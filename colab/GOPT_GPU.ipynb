{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOPT_GPU",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HninLwin-byte/ASR/blob/master/colab/GOPT_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment](https://arxiv.org/abs/2205.03432)\n",
        "\n",
        "- This colab script contains the official implementation and pretrained model (in PyTorch) of the **Goodness Of Pronunciation Feature-Based Transformer (GOPT)** proposed in the ICASSP 2022 paper [Transformer-Based Multi-Aspect Multi-Granularity Non-native English Speaker Pronunciation Assessment](https://ieeexplore.ieee.org/document/9746743) (Yuan Gong, Ziyi Chen, Iek-Heng Chu, Peng Chang, James Glass; MIT & PAII).\n",
        "\n",
        "- Please cite our paper if you find this repository useful.\n",
        "\n",
        " - ```\n",
        "@INPROCEEDINGS{gong_gopt,\n",
        "  author={Gong, Yuan and Chen, Ziyi and Chu, Iek-Heng and Chang, Peng and Glass, James},\n",
        "  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n",
        "  title={Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment},\n",
        "  year={2022},\n",
        "  pages={7262-7266},\n",
        "  doi={10.1109/ICASSP43922.2022.9746743}}\n",
        "```\n",
        "\n",
        "\n",
        "- For more information, please check https://github.com/YuanGongND/gopt"
      ],
      "metadata": {
        "collapsed": false,
        "id": "hbpzp6zbFCEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1. Automatically load the Kaldi GOP feature.\n",
        "Note: this skips the Kaldi GOP feature extraction part. If you are interested in the feature extraction, please check our GitHub on how to do that."
      ],
      "metadata": {
        "id": "kKg-0_ISru4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print('current working dir is ' + os.getcwd())\n",
        "data_dir = os.getcwd() + '/gopt'\n",
        "if os.path.exists(data_dir) == True:\n",
        "    print('data path already exists')\n",
        "else:\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "if os.path.exists(os.getcwd() + '/gopt/data.zip') == False:\n",
        "  print('Downloading the intermediate GOP feature, please be patient.')\n",
        "  os.system('wget https://www.dropbox.com/s/zc6o1d8rqq28vci/data.zip?dl=1 -O ' + os.getcwd() +'/gopt/data.zip')\n",
        "  os.system('unzip -q ' + os.getcwd() + '/gopt/data.zip -d ' + os.getcwd() + '/gopt/')\n",
        "  print('Kaldi GOP features loaded at ' + os.getcwd() + '/gopt/, check the fold button on the left hand for details.')\n",
        "else:\n",
        "  print('Kaldi GOP features already loadedat ' + os.getcwd() + '/gopt/, check the fold button on the left hand for details.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqFES3mXuA2n",
        "outputId": "5562c212-160d-45f5-b827-72788819c60c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current working dir is /content\n",
            "Downloading the intermediate GOP feature, please be patient.\n",
            "Kaldi GOP features loaded at /content/gopt/, check the fold button on the left hand for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Build the GOPT model."
      ],
      "metadata": {
        "id": "366AJg7CpXjP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7bNAIRSoKDp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# code from the t2t-vit paper\n",
        "def get_sinusoid_encoding(n_position, d_hid):\n",
        "    ''' Sinusoid position encoding table '''\n",
        "\n",
        "    def get_position_angle_vec(position):\n",
        "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        #print(C)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "# standard GOPT model proposed in the paper\n",
        "class GOPT(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, depth, input_dim=84):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        # Transformer encode blocks\n",
        "        self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads) for i in range(depth)])\n",
        "\n",
        "        # sin pos embedding or learnable pos embedding, 55 = 50 sequence length + 5 utt-level cls tokens\n",
        "        #self.pos_embed = nn.Parameter(get_sinusoid_encoding(55, self.embed_dim) * 0.1, requires_grad=True)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 55, self.embed_dim))\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "\n",
        "        # for phone classification\n",
        "        self.in_proj = nn.Linear(self.input_dim, embed_dim)\n",
        "        self.mlp_head_phn = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "\n",
        "        # for word classification, 1=accuracy, 2=stress, 3=total\n",
        "        self.mlp_head_word1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.mlp_head_word2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.mlp_head_word3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "\n",
        "        # canonical phone projection, assume there are 40 phns\n",
        "        self.phn_proj = nn.Linear(40, embed_dim)\n",
        "\n",
        "        # utterance level, 1=accuracy, 2=completeness, 3=fluency, 4=prosodic, 5=total score\n",
        "        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.mlp_head_utt1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.mlp_head_utt2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.mlp_head_utt3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.mlp_head_utt4 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "        self.cls_token5 = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.mlp_head_utt5 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
        "\n",
        "        # initialize the cls tokens\n",
        "        trunc_normal_(self.cls_token1, std=.02)\n",
        "        trunc_normal_(self.cls_token2, std=.02)\n",
        "        trunc_normal_(self.cls_token3, std=.02)\n",
        "        trunc_normal_(self.cls_token4, std=.02)\n",
        "        trunc_normal_(self.cls_token5, std=.02)\n",
        "\n",
        "    # x shape in [batch_size, sequence_len, feat_dim]\n",
        "    # phn in [batch_size, seq_len]\n",
        "    def forward(self, x, phn):\n",
        "\n",
        "        # batch size\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # phn_one_hot in shape [batch_size, seq_len, feat_dim]\n",
        "        phn_one_hot = torch.nn.functional.one_hot(phn.long()+1, num_classes=40).float()\n",
        "        # phn_embed in shape [batch_size, seq_len, embed_dim]\n",
        "        phn_embed = self.phn_proj(phn_one_hot)\n",
        "\n",
        "        # if the input dimension is different from the Transformer embedding dimension, project the input to same dim\n",
        "        if self.embed_dim != self.input_dim:\n",
        "            x = self.in_proj(x)\n",
        "\n",
        "        x = x + phn_embed\n",
        "\n",
        "        cls_token1 = self.cls_token1.expand(B, -1, -1)\n",
        "        cls_token2 = self.cls_token2.expand(B, -1, -1)\n",
        "        cls_token3 = self.cls_token3.expand(B, -1, -1)\n",
        "        cls_token4 = self.cls_token4.expand(B, -1, -1)\n",
        "        cls_token5 = self.cls_token5.expand(B, -1, -1)\n",
        "\n",
        "        x = torch.cat((cls_token1, cls_token2, cls_token3, cls_token4, cls_token5, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # forward to the Transformer encoder\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # the first 5 tokens are utterance-level cls tokens, i.e., accuracy, completeness, fluency, prosodic, total scores\n",
        "        u1 = self.mlp_head_utt1(x[:, 0])\n",
        "        u2 = self.mlp_head_utt2(x[:, 1])\n",
        "        u3 = self.mlp_head_utt3(x[:, 2])\n",
        "        u4 = self.mlp_head_utt4(x[:, 3])\n",
        "        u5 = self.mlp_head_utt5(x[:, 4])\n",
        "\n",
        "        # 6th-end tokens are phone score tokens\n",
        "        p = self.mlp_head_phn(x[:, 5:])\n",
        "\n",
        "        # word score is propagated to phone-level, so word output is also at phone-level.\n",
        "        # but different mlp heads are used, 1 = accuracy, 2 = stress, 3 = total\n",
        "        w1 = self.mlp_head_word1(x[:, 5:])\n",
        "        w2 = self.mlp_head_word2(x[:, 5:])\n",
        "        w3 = self.mlp_head_word3(x[:, 5:])\n",
        "        return u1, u2, u3, u4, u5, p, w1, w2, w3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Load the GOP features."
      ],
      "metadata": {
        "id": "Xf9OIkR1pvE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GoPDataset(Dataset):\n",
        "    def __init__(self, set, am='librispeech'):\n",
        "        # normalize the input to 0 mean and unit std.\n",
        "        if am=='librispeech':\n",
        "            dir='seq_data_librispeech'\n",
        "            norm_mean, norm_std = 3.203, 4.045\n",
        "        elif am=='paiia':\n",
        "            dir='seq_data_paiia'\n",
        "            norm_mean, norm_std = -0.652, 9.737\n",
        "        elif am=='paiib':\n",
        "            dir='seq_data_paiib'\n",
        "            norm_mean, norm_std = -0.516, 9.247\n",
        "        else:\n",
        "            raise ValueError('Acoustic Model Unrecognized.')\n",
        "\n",
        "        if set == 'train':\n",
        "            self.feat = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/tr_feat.npy'), dtype=torch.float)\n",
        "            self.phn_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/tr_label_phn.npy'), dtype=torch.float)\n",
        "            self.utt_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/tr_label_utt.npy'), dtype=torch.float)\n",
        "            self.word_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/tr_label_word.npy'), dtype=torch.float)\n",
        "        elif set == 'test':\n",
        "            self.feat = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/te_feat.npy'), dtype=torch.float)\n",
        "            self.phn_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/te_label_phn.npy'), dtype=torch.float)\n",
        "            self.utt_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/te_label_utt.npy'), dtype=torch.float)\n",
        "            self.word_label = torch.tensor(np.load(os.getcwd()+'/gopt/'+dir+'/te_label_word.npy'), dtype=torch.float)\n",
        "\n",
        "        # normalize the GOP feature using the training set mean and std (only count the valid token features, exclude the padded tokens).\n",
        "        self.feat = self.norm_valid(self.feat, norm_mean, norm_std)\n",
        "\n",
        "        # normalize the utt_label to 0-2 (same with phn score range)\n",
        "        self.utt_label = self.utt_label / 5\n",
        "        # the last dim is word_id, so not normalizing\n",
        "        self.word_label[:, :, 0:3] = self.word_label[:, :, 0:3] / 5\n",
        "        self.phn_label[:, :, 1] = self.phn_label[:, :, 1]\n",
        "\n",
        "    # only normalize valid tokens, not padded token\n",
        "    def norm_valid(self, feat, norm_mean, norm_std):\n",
        "        norm_feat = torch.zeros_like(feat)\n",
        "        for i in range(feat.shape[0]):\n",
        "            for j in range(feat.shape[1]):\n",
        "                if feat[i, j, 0] != 0:\n",
        "                    norm_feat[i, j, :] = (feat[i, j, :] - norm_mean) / norm_std\n",
        "                else:\n",
        "                    break\n",
        "        return norm_feat\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.feat.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # feat, phn_label, phn_id, utt_label, word_label\n",
        "        return self.feat[idx, :], self.phn_label[idx, :, 1], self.phn_label[idx, :, 0], self.utt_label[idx, :], self.word_label[idx, :]"
      ],
      "metadata": {
        "id": "Z5z2U2dTpuNQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Build the training and evaluation pipeline."
      ],
      "metadata": {
        "id": "dXHf-H1jvzOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "print(\"I am process %s, running on %s: starting (%s)\" % (os.getpid(), os.uname()[1], time.asctime()))\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument(\"--exp-dir\", type=str, default=os.getcwd()+\"/exp/\", help=\"directory to dump experiments\")\n",
        "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, metavar='LR', help='initial learning rate')\n",
        "parser.add_argument(\"--n-epochs\", type=int, default=50, help=\"number of maximum training epochs\")\n",
        "parser.add_argument(\"--goptdepth\", type=int, default=3, help=\"depth of gopt models\")\n",
        "parser.add_argument(\"--goptheads\", type=int, default=1, help=\"heads of gopt models\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=25, help=\"training batch size\")\n",
        "parser.add_argument(\"--embed_dim\", type=int, default=24, help=\"gopt transformer embedding dimension\")\n",
        "parser.add_argument(\"--loss_w_phn\", type=float, default=1, help=\"weight for phoneme-level loss\")\n",
        "parser.add_argument(\"--loss_w_word\", type=float, default=1, help=\"weight for word-level loss\")\n",
        "parser.add_argument(\"--loss_w_utt\", type=float, default=1, help=\"weight for utterance-level loss\")\n",
        "parser.add_argument(\"--model\", type=str, default='gopt', help=\"name of the model\")\n",
        "parser.add_argument(\"--am\", type=str, default='paiia', help=\"name of the acoustic models\")\n",
        "parser.add_argument(\"--noise\", type=float, default=0., help=\"the scale of random noise added on the input GoP feature\")\n",
        "\n",
        "# just to generate the header for the result.csv\n",
        "def gen_result_header():\n",
        "    phn_header = ['epoch', 'phone_train_mse', 'phone_train_pcc', 'phone_test_mse', 'phone_test_pcc', 'learning rate']\n",
        "    utt_header_set = ['utt_train_mse', 'utt_train_pcc', 'utt_test_mse', 'utt_test_pcc']\n",
        "    utt_header_score = ['accuracy', 'completeness', 'fluency', 'prosodic', 'total']\n",
        "    word_header_set = ['word_train_pcc', 'word_test_pcc']\n",
        "    word_header_score = ['accuracy', 'stress', 'total']\n",
        "    utt_header, word_header = [], []\n",
        "    for dset in utt_header_set:\n",
        "        utt_header = utt_header + [dset+'_'+x for x in utt_header_score]\n",
        "    for dset in word_header_set:\n",
        "        word_header = word_header + [dset+'_'+x for x in word_header_score]\n",
        "    header = phn_header + utt_header + word_header\n",
        "    return header\n",
        "\n",
        "def train(audio_model, train_loader, test_loader, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('running on ' + str(device))\n",
        "\n",
        "    # best_cum_mAP is checkpoint ensemble from the first epoch to the best epoch\n",
        "    best_epoch, best_mse = 0, 999\n",
        "    global_step, epoch = 0, 0\n",
        "    exp_dir = args.exp_dir\n",
        "\n",
        "    if not isinstance(audio_model, nn.DataParallel):\n",
        "        audio_model = nn.DataParallel(audio_model)\n",
        "\n",
        "    audio_model = audio_model.to(device)\n",
        "    # Set up the optimizer\n",
        "    trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
        "    print('Total parameter number is : {:.3f} k'.format(sum(p.numel() for p in audio_model.parameters()) / 1e3))\n",
        "    print('Total trainable parameter number is : {:.3f} k'.format(sum(p.numel() for p in trainables) / 1e3))\n",
        "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(10, 100, 5)), gamma=0.5, last_epoch=-1)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
        "    print(\"start training...\")\n",
        "    result = np.zeros([args.n_epochs, 32])\n",
        "\n",
        "    while epoch < args.n_epochs:\n",
        "        audio_model.train()\n",
        "        for i, (audio_input, phn_label, phns, utt_label, word_label) in enumerate(train_loader):\n",
        "\n",
        "            audio_input = audio_input.to(device, non_blocking=True)\n",
        "            phn_label = phn_label.to(device, non_blocking=True)\n",
        "            utt_label = utt_label.to(device, non_blocking=True)\n",
        "            word_label = word_label.to(device, non_blocking=True)\n",
        "\n",
        "            # warmup\n",
        "            warm_up_step = 100\n",
        "            if global_step <= warm_up_step and global_step % 5 == 0:\n",
        "                warm_lr = (global_step / warm_up_step) * args.lr\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = warm_lr\n",
        "                #print('warm-up learning rate is {:f}'.format(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "            # add random noise for augmentation.\n",
        "            noise = (torch.rand([audio_input.shape[0], audio_input.shape[1], audio_input.shape[2]]) - 1) * args.noise\n",
        "            noise = noise.to(device, non_blocking=True)\n",
        "            audio_input = audio_input + noise\n",
        "\n",
        "            #print(phns.shape)\n",
        "            u1, u2, u3, u4, u5, p, w1, w2, w3 = audio_model(audio_input, phns)\n",
        "\n",
        "            # filter out the padded tokens, only calculate the loss based on the valid tokens\n",
        "            # < 0 is a flag of padded tokens\n",
        "            mask = (phn_label>=0)\n",
        "            p = p.squeeze(2)\n",
        "            p = p * mask\n",
        "            phn_label = phn_label * mask\n",
        "\n",
        "            loss_phn = loss_fn(p, phn_label)\n",
        "\n",
        "            # avoid the 0 losses of the padded tokens impacting the performance\n",
        "            loss_phn = loss_phn * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
        "\n",
        "            # utterance level loss, also mse\n",
        "            utt_preds = torch.cat((u1, u2, u3, u4, u5), dim=1)\n",
        "            loss_utt = loss_fn(utt_preds ,utt_label)\n",
        "\n",
        "            # word level loss\n",
        "            word_label = word_label[:, :, 0:3]\n",
        "            mask = (word_label>=0)\n",
        "            word_pred = torch.cat((w1,w2,w3), dim=2)\n",
        "            word_pred = word_pred * mask\n",
        "            word_label = word_label * mask\n",
        "            loss_word = loss_fn(word_pred, word_label)\n",
        "            loss_word = loss_word * (mask.shape[0] * mask.shape[1] * mask.shape[2]) / torch.sum(mask)\n",
        "\n",
        "            loss = args.loss_w_phn * loss_phn + args.loss_w_utt * loss_utt + args.loss_w_word * loss_word\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_step += 1\n",
        "\n",
        "        print('start validation of epoch {:d}'.format(epoch))\n",
        "\n",
        "        # ensemble results\n",
        "        # don't save prediction for the training set\n",
        "        tr_mse, tr_corr, tr_utt_mse, tr_utt_corr, tr_word_mse, tr_word_corr = validate(audio_model, train_loader, args, -1)\n",
        "        te_mse, te_corr, te_utt_mse, te_utt_corr, te_word_mse, te_word_corr = validate(audio_model, test_loader, args, best_mse)\n",
        "\n",
        "        print('Phone: Test MSE: {:.3f}, CORR: {:.3f}'.format(te_mse.item(), te_corr))\n",
        "        print('Utterance:, ACC: {:.3f}, COM: {:.3f}, FLU: {:.3f}, PROC: {:.3f}, Total: {:.3f}'.format(te_utt_corr[0], te_utt_corr[1], te_utt_corr[2], te_utt_corr[3], te_utt_corr[4]))\n",
        "        print('Word:, ACC: {:.3f}, Stress: {:.3f}, Total: {:.3f}'.format(te_word_corr[0], te_word_corr[1], te_word_corr[2]))\n",
        "\n",
        "        result[epoch, :6] = [epoch, tr_mse, tr_corr, te_mse, te_corr, optimizer.param_groups[0]['lr']]\n",
        "\n",
        "        result[epoch, 6:26] = np.concatenate([tr_utt_mse, tr_utt_corr, te_utt_mse, te_utt_corr])\n",
        "\n",
        "        result[epoch, 26:32] = np.concatenate([tr_word_corr, te_word_corr])\n",
        "\n",
        "        header = ','.join(gen_result_header())\n",
        "        np.savetxt(exp_dir + '/result.csv', result, delimiter=',', header=header, comments='')\n",
        "        print('-------------------validation finished-------------------')\n",
        "\n",
        "        if te_mse < best_mse:\n",
        "            best_mse = te_mse\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if best_epoch == epoch:\n",
        "            if os.path.exists(\"%s/models/\" % (exp_dir)) == False:\n",
        "                os.mkdir(\"%s/models\" % (exp_dir))\n",
        "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
        "\n",
        "        if global_step > warm_up_step:\n",
        "            scheduler.step()\n",
        "\n",
        "        #print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "        epoch += 1\n",
        "\n",
        "def validate(audio_model, val_loader, args, best_mse):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if not isinstance(audio_model, nn.DataParallel):\n",
        "        audio_model = nn.DataParallel(audio_model)\n",
        "    audio_model = audio_model.to(device)\n",
        "    audio_model.eval()\n",
        "\n",
        "    A_phn, A_phn_target = [], []\n",
        "    A_u1, A_u2, A_u3, A_u4, A_u5, A_utt_target = [], [], [], [], [], []\n",
        "    A_w1, A_w2, A_w3, A_word_target = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (audio_input, phn_label, phns, utt_label, word_label) in enumerate(val_loader):\n",
        "            audio_input = audio_input.to(device)\n",
        "\n",
        "            # compute output\n",
        "            u1, u2, u3, u4, u5, p, w1, w2, w3 = audio_model(audio_input, phns)\n",
        "            p = p.to('cpu').detach()\n",
        "            u1, u2, u3, u4, u5 = u1.to('cpu').detach(), u2.to('cpu').detach(), u3.to('cpu').detach(), u4.to('cpu').detach(), u5.to('cpu').detach()\n",
        "            w1, w2, w3 = w1.to('cpu').detach(), w2.to('cpu').detach(), w3.to('cpu').detach()\n",
        "\n",
        "            A_phn.append(p)\n",
        "            A_phn_target.append(phn_label)\n",
        "\n",
        "            A_u1.append(u1)\n",
        "            A_u2.append(u2)\n",
        "            A_u3.append(u3)\n",
        "            A_u4.append(u4)\n",
        "            A_u5.append(u5)\n",
        "            A_utt_target.append(utt_label)\n",
        "\n",
        "            A_w1.append(w1)\n",
        "            A_w2.append(w2)\n",
        "            A_w3.append(w3)\n",
        "            A_word_target.append(word_label)\n",
        "\n",
        "        # phone level\n",
        "        A_phn, A_phn_target  = torch.cat(A_phn), torch.cat(A_phn_target)\n",
        "\n",
        "        # utterance level\n",
        "        A_u1, A_u2, A_u3, A_u4, A_u5, A_utt_target = torch.cat(A_u1), torch.cat(A_u2), torch.cat(A_u3), torch.cat(A_u4), torch.cat(A_u5), torch.cat(A_utt_target)\n",
        "\n",
        "        # word level\n",
        "        A_w1, A_w2, A_w3, A_word_target = torch.cat(A_w1), torch.cat(A_w2), torch.cat(A_w3), torch.cat(A_word_target)\n",
        "\n",
        "        # get the scores\n",
        "        phn_mse, phn_corr = valid_phn(A_phn, A_phn_target)\n",
        "\n",
        "        A_utt = torch.cat((A_u1, A_u2, A_u3, A_u4, A_u5), dim=1)\n",
        "        utt_mse, utt_corr = valid_utt(A_utt, A_utt_target)\n",
        "\n",
        "        A_word = torch.cat((A_w1, A_w2, A_w3), dim=2)\n",
        "        word_mse, word_corr, valid_word_pred, valid_word_target = valid_word(A_word, A_word_target)\n",
        "\n",
        "        if phn_mse < best_mse:\n",
        "            print('new best phn mse {:.3f}, now saving predictions.'.format(phn_mse))\n",
        "\n",
        "            # create the directory\n",
        "            if os.path.exists(args.exp_dir + '/preds') == False:\n",
        "                os.mkdir(args.exp_dir + '/preds')\n",
        "\n",
        "            # saving the phn target, only do once\n",
        "            if os.path.exists(args.exp_dir + '/preds/phn_target.npy') == False:\n",
        "                np.save(args.exp_dir + '/preds/phn_target.npy', A_phn_target)\n",
        "                np.save(args.exp_dir + '/preds/word_target.npy', valid_word_target)\n",
        "                np.save(args.exp_dir + '/preds/utt_target.npy', A_utt_target)\n",
        "\n",
        "            np.save(args.exp_dir + '/preds/phn_pred.npy', A_phn)\n",
        "            np.save(args.exp_dir + '/preds/word_pred.npy', valid_word_pred)\n",
        "            np.save(args.exp_dir + '/preds/utt_pred.npy', A_utt)\n",
        "\n",
        "    return phn_mse, phn_corr, utt_mse, utt_corr, word_mse, word_corr\n",
        "\n",
        "def valid_phn(audio_output, target):\n",
        "    valid_token_pred = []\n",
        "    valid_token_target = []\n",
        "    audio_output = audio_output.squeeze(2)\n",
        "    for i in range(audio_output.shape[0]):\n",
        "        for j in range(audio_output.shape[1]):\n",
        "            # only count valid tokens, not padded tokens (represented by negative values)\n",
        "            if target[i, j] >= 0:\n",
        "                valid_token_pred.append(audio_output[i, j])\n",
        "                valid_token_target.append(target[i, j])\n",
        "    valid_token_target = np.array(valid_token_target)\n",
        "    valid_token_pred = np.array(valid_token_pred)\n",
        "\n",
        "    valid_token_mse = np.mean((valid_token_target - valid_token_pred) ** 2)\n",
        "    corr = np.corrcoef(valid_token_pred, valid_token_target)[0, 1]\n",
        "    return valid_token_mse, corr\n",
        "\n",
        "def valid_utt(audio_output, target):\n",
        "    mse = []\n",
        "    corr = []\n",
        "    for i in range(5):\n",
        "        cur_mse = np.mean(((audio_output[:, i] - target[:, i]) ** 2).numpy())\n",
        "        cur_corr = np.corrcoef(audio_output[:, i], target[:, i])[0, 1]\n",
        "        mse.append(cur_mse)\n",
        "        corr.append(cur_corr)\n",
        "    return mse, corr\n",
        "\n",
        "def valid_word(audio_output, target):\n",
        "    word_id = target[:, :, -1]\n",
        "    target = target[:, :, 0:3]\n",
        "\n",
        "    valid_token_pred = []\n",
        "    valid_token_target = []\n",
        "\n",
        "    # unique, counts = np.unique(np.array(target), return_counts=True)\n",
        "    # print(dict(zip(unique, counts)))\n",
        "\n",
        "    # for each utterance\n",
        "    for i in range(target.shape[0]):\n",
        "        prev_w_id = 0\n",
        "        start_id = 0\n",
        "        # for each token\n",
        "        for j in range(target.shape[1]):\n",
        "            cur_w_id = word_id[i, j].int()\n",
        "            # if a new word\n",
        "            if cur_w_id != prev_w_id:\n",
        "                # average each phone belongs to the word\n",
        "                valid_token_pred.append(np.mean(audio_output[i, start_id: j, :].numpy(), axis=0))\n",
        "                valid_token_target.append(np.mean(target[i, start_id: j, :].numpy(), axis=0))\n",
        "                # sanity check, if the range indeed contains a single word\n",
        "                if len(torch.unique(target[i, start_id: j, 1])) != 1:\n",
        "                    print(target[i, start_id: j, 0])\n",
        "                # if end of the utterance\n",
        "                if cur_w_id == -1:\n",
        "                    break\n",
        "                else:\n",
        "                    prev_w_id = cur_w_id\n",
        "                    start_id = j\n",
        "\n",
        "    valid_token_pred = np.array(valid_token_pred)\n",
        "    # this rounding is to solve the precision issue in the label\n",
        "    valid_token_target = np.array(valid_token_target).round(2)\n",
        "\n",
        "    mse_list, corr_list = [], []\n",
        "    # for each (accuracy, stress, total) word score\n",
        "    for i in range(3):\n",
        "        valid_token_mse = np.mean((valid_token_target[:, i] - valid_token_pred[:, i]) ** 2)\n",
        "        corr = np.corrcoef(valid_token_pred[:, i], valid_token_target[:, i])[0, 1]\n",
        "        mse_list.append(valid_token_mse)\n",
        "        corr_list.append(corr)\n",
        "    return mse_list, corr_list, valid_token_pred, valid_token_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjCZLezhvxfB",
        "outputId": "8cddc180-f550-426b-dc4f-93f8e2bdb54d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am process 480, running on fb6123f31df5: starting (Sun Jun 29 07:09:24 2025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. Train the model and see the results!\n",
        "Note: due to various reasons, the colab result will be slightly different with (but still very close to) that using the local script. E.g., in this run, we get 0.682 phone-level PCC, 0.600 word-level PCC, and 0.729 utterance-level PCC, while in the paper, we report 0.679. 0.601, and 0.731, respectively."
      ],
      "metadata": {
        "id": "UtIUrJo0OHEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args(args=[])\n",
        "\n",
        "if torch.cuda.is_available() == False:\n",
        "    raise ValueError('GPU is not enabled. Please go to top menu - edit - notebook settings -hardware accelerator - GPU')\n",
        "\n",
        "am = args.am\n",
        "print('now train with {:s} acoustic models'.format(am))\n",
        "feat_dim = {'librispeech':84, 'paiia':86, 'paiib': 88}\n",
        "input_dim=feat_dim[am]\n",
        "\n",
        "# nowa is the best models used in this work\n",
        "if args.model == 'gopt':\n",
        "    print('now train a GOPT models')\n",
        "    audio_mdl = GOPT(embed_dim=args.embed_dim, num_heads=args.goptheads, depth=args.goptdepth, input_dim=input_dim)\n",
        "\n",
        "tr_dataset = GoPDataset('train', am=am)\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "te_dataset = GoPDataset('test', am=am)\n",
        "te_dataloader = DataLoader(te_dataset, batch_size=2500, shuffle=False)\n",
        "\n",
        "if os.path.exists(args.exp_dir) == False:\n",
        "  os.makedirs(args.exp_dir)\n",
        "train(audio_mdl, tr_dataloader, te_dataloader, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOEATqXrwkqn",
        "outputId": "d0957691-e65d-487d-8f9c-400d8778e833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now train with paiia acoustic models\n",
            "now train a GOPT models\n",
            "running on cuda\n",
            "Total parameter number is : 26.625 k\n",
            "Total trainable parameter number is : 26.625 k\n",
            "current #steps=0, #epochs=0\n",
            "start training...\n",
            "start validation of epoch 0\n",
            "new best phn mse 0.110, now saving predictions.\n",
            "Phone: Test MSE: 0.110, CORR: 0.411\n",
            "Utterance:, ACC: 0.100, COM: 0.054, FLU: -0.098, PROC: 0.005, Total: 0.105\n",
            "Word:, ACC: 0.288, Stress: 0.107, Total: -0.197\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 1\n",
            "new best phn mse 0.100, now saving predictions.\n",
            "Phone: Test MSE: 0.100, CORR: 0.492\n",
            "Utterance:, ACC: 0.342, COM: 0.074, FLU: -0.022, PROC: 0.096, Total: 0.452\n",
            "Word:, ACC: 0.426, Stress: 0.141, Total: 0.436\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 2\n",
            "new best phn mse 0.092, now saving predictions.\n",
            "Phone: Test MSE: 0.092, CORR: 0.535\n",
            "Utterance:, ACC: 0.273, COM: 0.102, FLU: 0.060, PROC: 0.120, Total: 0.531\n",
            "Word:, ACC: 0.478, Stress: 0.129, Total: 0.509\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 3\n",
            "new best phn mse 0.090, now saving predictions.\n",
            "Phone: Test MSE: 0.090, CORR: 0.571\n",
            "Utterance:, ACC: 0.327, COM: 0.143, FLU: 0.150, PROC: 0.172, Total: 0.472\n",
            "Word:, ACC: 0.516, Stress: 0.096, Total: 0.551\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 4\n",
            "new best phn mse 0.078, now saving predictions.\n",
            "Phone: Test MSE: 0.078, CORR: 0.621\n",
            "Utterance:, ACC: 0.611, COM: 0.171, FLU: 0.502, PROC: 0.460, Total: 0.631\n",
            "Word:, ACC: 0.556, Stress: 0.116, Total: 0.590\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 5\n",
            "new best phn mse 0.074, now saving predictions.\n",
            "Phone: Test MSE: 0.074, CORR: 0.644\n",
            "Utterance:, ACC: 0.670, COM: 0.096, FLU: 0.601, PROC: 0.602, Total: 0.683\n",
            "Word:, ACC: 0.577, Stress: 0.109, Total: 0.583\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 6\n",
            "Phone: Test MSE: 0.078, CORR: 0.655\n",
            "Utterance:, ACC: 0.650, COM: -0.019, FLU: 0.563, PROC: 0.608, Total: 0.683\n",
            "Word:, ACC: 0.561, Stress: 0.121, Total: 0.563\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 7\n",
            "new best phn mse 0.071, now saving predictions.\n",
            "Phone: Test MSE: 0.071, CORR: 0.664\n",
            "Utterance:, ACC: 0.699, COM: 0.008, FLU: 0.627, PROC: 0.638, Total: 0.709\n",
            "Word:, ACC: 0.575, Stress: 0.143, Total: 0.588\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 8\n",
            "Phone: Test MSE: 0.072, CORR: 0.667\n",
            "Utterance:, ACC: 0.714, COM: 0.092, FLU: 0.644, PROC: 0.644, Total: 0.712\n",
            "Word:, ACC: 0.575, Stress: 0.178, Total: 0.588\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 9\n",
            "new best phn mse 0.070, now saving predictions.\n",
            "Phone: Test MSE: 0.070, CORR: 0.672\n",
            "Utterance:, ACC: 0.715, COM: 0.007, FLU: 0.645, PROC: 0.645, Total: 0.719\n",
            "Word:, ACC: 0.587, Stress: 0.163, Total: 0.598\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 10\n",
            "Phone: Test MSE: 0.071, CORR: 0.668\n",
            "Utterance:, ACC: 0.686, COM: -0.056, FLU: 0.611, PROC: 0.619, Total: 0.702\n",
            "Word:, ACC: 0.582, Stress: 0.158, Total: 0.586\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 11\n",
            "new best phn mse 0.069, now saving predictions.\n",
            "Phone: Test MSE: 0.069, CORR: 0.675\n",
            "Utterance:, ACC: 0.711, COM: -0.003, FLU: 0.640, PROC: 0.639, Total: 0.718\n",
            "Word:, ACC: 0.589, Stress: 0.172, Total: 0.598\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 12\n",
            "Phone: Test MSE: 0.072, CORR: 0.672\n",
            "Utterance:, ACC: 0.719, COM: 0.016, FLU: 0.654, PROC: 0.650, Total: 0.720\n",
            "Word:, ACC: 0.586, Stress: 0.171, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 13\n",
            "new best phn mse 0.069, now saving predictions.\n",
            "Phone: Test MSE: 0.069, CORR: 0.678\n",
            "Utterance:, ACC: 0.718, COM: -0.031, FLU: 0.652, PROC: 0.649, Total: 0.723\n",
            "Word:, ACC: 0.586, Stress: 0.172, Total: 0.595\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 14\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.678\n",
            "Utterance:, ACC: 0.713, COM: 0.002, FLU: 0.650, PROC: 0.648, Total: 0.721\n",
            "Word:, ACC: 0.589, Stress: 0.176, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 15\n",
            "Phone: Test MSE: 0.069, CORR: 0.681\n",
            "Utterance:, ACC: 0.716, COM: -0.065, FLU: 0.652, PROC: 0.651, Total: 0.722\n",
            "Word:, ACC: 0.595, Stress: 0.173, Total: 0.604\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 16\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.722, COM: 0.026, FLU: 0.658, PROC: 0.655, Total: 0.726\n",
            "Word:, ACC: 0.587, Stress: 0.159, Total: 0.597\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 17\n",
            "Phone: Test MSE: 0.069, CORR: 0.678\n",
            "Utterance:, ACC: 0.723, COM: -0.013, FLU: 0.661, PROC: 0.659, Total: 0.727\n",
            "Word:, ACC: 0.587, Stress: 0.173, Total: 0.596\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 18\n",
            "Phone: Test MSE: 0.069, CORR: 0.680\n",
            "Utterance:, ACC: 0.722, COM: 0.011, FLU: 0.659, PROC: 0.655, Total: 0.725\n",
            "Word:, ACC: 0.593, Stress: 0.173, Total: 0.604\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 19\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.722, COM: 0.001, FLU: 0.659, PROC: 0.658, Total: 0.727\n",
            "Word:, ACC: 0.590, Stress: 0.166, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 20\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.719, COM: -0.008, FLU: 0.658, PROC: 0.657, Total: 0.725\n",
            "Word:, ACC: 0.587, Stress: 0.173, Total: 0.596\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 21\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.720, COM: -0.016, FLU: 0.659, PROC: 0.658, Total: 0.725\n",
            "Word:, ACC: 0.592, Stress: 0.171, Total: 0.603\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 22\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.724, COM: -0.000, FLU: 0.664, PROC: 0.662, Total: 0.728\n",
            "Word:, ACC: 0.589, Stress: 0.169, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 23\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.725, COM: -0.003, FLU: 0.664, PROC: 0.662, Total: 0.728\n",
            "Word:, ACC: 0.591, Stress: 0.170, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 24\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.662, PROC: 0.662, Total: 0.727\n",
            "Word:, ACC: 0.592, Stress: 0.171, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 25\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.002, FLU: 0.666, PROC: 0.665, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.169, Total: 0.602\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 26\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: -0.022, FLU: 0.663, PROC: 0.663, Total: 0.727\n",
            "Word:, ACC: 0.590, Stress: 0.170, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 27\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.725, COM: -0.003, FLU: 0.666, PROC: 0.665, Total: 0.729\n",
            "Word:, ACC: 0.589, Stress: 0.171, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 28\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.725, COM: -0.003, FLU: 0.667, PROC: 0.667, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.163, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 29\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.005, FLU: 0.667, PROC: 0.667, Total: 0.729\n",
            "Word:, ACC: 0.592, Stress: 0.172, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 30\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: -0.012, FLU: 0.665, PROC: 0.665, Total: 0.728\n",
            "Word:, ACC: 0.592, Stress: 0.167, Total: 0.602\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 31\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.002, FLU: 0.668, PROC: 0.667, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.169, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 32\n",
            "Phone: Test MSE: 0.068, CORR: 0.681\n",
            "Utterance:, ACC: 0.725, COM: -0.000, FLU: 0.668, PROC: 0.668, Total: 0.729\n",
            "Word:, ACC: 0.589, Stress: 0.171, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 33\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: -0.016, FLU: 0.666, PROC: 0.666, Total: 0.728\n",
            "Word:, ACC: 0.591, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 34\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.725, COM: -0.008, FLU: 0.668, PROC: 0.668, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 35\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.725, COM: -0.017, FLU: 0.668, PROC: 0.668, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.167, Total: 0.601\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 36\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.005, FLU: 0.669, PROC: 0.668, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 37\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.008, FLU: 0.668, PROC: 0.668, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 38\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.006, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 39\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.004, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 40\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.002, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.169, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 41\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.724, COM: -0.011, FLU: 0.668, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.591, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 42\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.005, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 43\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.006, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 44\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.724, COM: -0.011, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 45\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.724, COM: -0.010, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 46\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.007, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 47\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.007, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.167, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 48\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.006, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.167, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 49\n",
            "Phone: Test MSE: 0.068, CORR: 0.682\n",
            "Utterance:, ACC: 0.725, COM: -0.009, FLU: 0.669, PROC: 0.669, Total: 0.729\n",
            "Word:, ACC: 0.590, Stress: 0.168, Total: 0.600\n",
            "-------------------validation finished-------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import subprocess\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "\n",
        "# Import your existing model classes (assuming they're in the same file or imported)\n",
        "# from your_model_file import GOPT, get_sinusoid_encoding, trunc_normal_, Attention, Mlp, Block\n",
        "\n",
        "class GOPInference:\n",
        "    def __init__(self, model_path, am='paiia', device='cuda'):\n",
        "        \"\"\"\n",
        "        Initialize GOP inference class\n",
        "\n",
        "        Args:\n",
        "            model_path: path to the trained model (.pth file)\n",
        "            am: acoustic model type ('librispeech', 'paiia', 'paiib')\n",
        "            device: computation device ('cuda' or 'cpu')\n",
        "        \"\"\"\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.am = am\n",
        "\n",
        "        # Set feature dimensions and normalization parameters\n",
        "        self.feat_dim = {'librispeech': 84, 'paiia': 86, 'paiib': 88}\n",
        "        self.norm_params = {\n",
        "            'librispeech': {'mean': 3.203, 'std': 4.045},\n",
        "            'paiia': {'mean': -0.652, 'std': 9.737},\n",
        "            'paiib': {'mean': -0.516, 'std': 9.247}\n",
        "        }\n",
        "\n",
        "        self.input_dim = self.feat_dim[am]\n",
        "        self.norm_mean = self.norm_params[am]['mean']\n",
        "        self.norm_std = self.norm_params[am]['std']\n",
        "\n",
        "        # Load the model\n",
        "        self.model = self.load_model(model_path)\n",
        "\n",
        "    def load_model(self, model_path, embed_dim=24, num_heads=1, depth=3):\n",
        "        \"\"\"Load the trained GOP model\"\"\"\n",
        "        model = GOPT(embed_dim=embed_dim, num_heads=num_heads, depth=depth, input_dim=self.input_dim)\n",
        "\n",
        "        # Load state dict\n",
        "        state_dict = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "        # Remove 'module.' prefix if model was trained with DataParallel\n",
        "        if list(state_dict.keys())[0].startswith('module.'):\n",
        "            state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def extract_kaldi_features(self, audio_file, text_file=None):\n",
        "        \"\"\"\n",
        "        Extract Kaldi GOP features from audio file\n",
        "        This is a placeholder - you'll need to implement actual Kaldi feature extraction\n",
        "\n",
        "        Args:\n",
        "            audio_file: path to audio file\n",
        "            text_file: path to text file with phoneme alignment (optional)\n",
        "\n",
        "        Returns:\n",
        "            features: numpy array of shape [seq_len, feat_dim]\n",
        "            phonemes: numpy array of phoneme indices\n",
        "        \"\"\"\n",
        "        # This is a placeholder implementation\n",
        "        # In practice, you would need to:\n",
        "        # 1. Run Kaldi ASR to get forced alignment\n",
        "        # 2. Extract GOP features using Kaldi tools\n",
        "        # 3. Process the features into the required format\n",
        "\n",
        "        print(\"WARNING: This is a placeholder for Kaldi feature extraction\")\n",
        "        print(\"You need to implement actual Kaldi GOP feature extraction\")\n",
        "\n",
        "        # Mock features for demonstration\n",
        "        seq_len = 50  # Adjust based on your audio length\n",
        "        features = np.random.randn(seq_len, self.input_dim)\n",
        "        phonemes = np.random.randint(0, 39, seq_len)  # Random phoneme indices\n",
        "\n",
        "        return features, phonemes\n",
        "\n",
        "    def preprocess_features(self, features):\n",
        "        \"\"\"Normalize features using training set statistics\"\"\"\n",
        "        # Only normalize valid tokens (non-zero features)\n",
        "        normalized_features = np.zeros_like(features)\n",
        "\n",
        "        for i in range(features.shape[0]):\n",
        "            if np.any(features[i] != 0):  # Valid token\n",
        "                normalized_features[i] = (features[i] - self.norm_mean) / self.norm_std\n",
        "            else:  # Padded token\n",
        "                break\n",
        "\n",
        "        return normalized_features\n",
        "\n",
        "    def pad_sequence(self, features, phonemes, max_len=50):\n",
        "        \"\"\"Pad sequences to fixed length\"\"\"\n",
        "        current_len = features.shape[0]\n",
        "\n",
        "        if current_len > max_len:\n",
        "            features = features[:max_len]\n",
        "            phonemes = phonemes[:max_len]\n",
        "        elif current_len < max_len:\n",
        "            # Pad with zeros\n",
        "            pad_len = max_len - current_len\n",
        "            features = np.vstack([features, np.zeros((pad_len, features.shape[1]))])\n",
        "            phonemes = np.concatenate([phonemes, np.full(pad_len, -1)])  # -1 for padding\n",
        "\n",
        "        return features, phonemes\n",
        "\n",
        "    def predict_single_audio(self, audio_file, text_file=None):\n",
        "        \"\"\"\n",
        "        Predict GOP scores for a single audio file\n",
        "\n",
        "        Args:\n",
        "            audio_file: path to audio file\n",
        "            text_file: path to text file with reference text (optional)\n",
        "\n",
        "        Returns:\n",
        "            results: dictionary containing all prediction scores\n",
        "        \"\"\"\n",
        "        # Extract Kaldi features\n",
        "        features, phonemes = self.extract_kaldi_features(audio_file, text_file)\n",
        "\n",
        "        # Preprocess features\n",
        "        features = self.preprocess_features(features)\n",
        "\n",
        "        # Pad to fixed length\n",
        "        features, phonemes = self.pad_sequence(features, phonemes)\n",
        "\n",
        "        # Convert to tensors\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "        phonemes_tensor = torch.tensor(phonemes, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Move to device\n",
        "        features_tensor = features_tensor.to(self.device)\n",
        "        phonemes_tensor = phonemes_tensor.to(self.device)\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            u1, u2, u3, u4, u5, p, w1, w2, w3 = self.model(features_tensor, phonemes_tensor)\n",
        "\n",
        "            # Convert to numpy and remove batch dimension\n",
        "            utterance_scores = {\n",
        "                'accuracy': u1.cpu().numpy()[0, 0] * 5,  # Denormalize (multiply by 5)\n",
        "                'completeness': u2.cpu().numpy()[0, 0] * 5,\n",
        "                'fluency': u3.cpu().numpy()[0, 0] * 5,\n",
        "                'prosodic': u4.cpu().numpy()[0, 0] * 5,\n",
        "                'total': u5.cpu().numpy()[0, 0] * 5\n",
        "            }\n",
        "\n",
        "            # Phone-level scores (only valid tokens)\n",
        "            phone_scores = p.cpu().numpy()[0, :, 0]\n",
        "            valid_phones = phonemes.cpu().numpy()[0] >= 0\n",
        "            phone_scores = phone_scores[valid_phones]\n",
        "\n",
        "            # Word-level scores (only valid tokens)\n",
        "            word_accuracy = w1.cpu().numpy()[0, :, 0] * 5\n",
        "            word_stress = w2.cpu().numpy()[0, :, 0] * 5\n",
        "            word_total = w3.cpu().numpy()[0, :, 0] * 5\n",
        "\n",
        "            word_scores = {\n",
        "                'accuracy': word_accuracy[valid_phones],\n",
        "                'stress': word_stress[valid_phones],\n",
        "                'total': word_total[valid_phones]\n",
        "            }\n",
        "\n",
        "        results = {\n",
        "            'utterance_scores': utterance_scores,\n",
        "            'phone_scores': phone_scores.tolist(),\n",
        "            'word_scores': word_scores,\n",
        "            'phonemes': phonemes.cpu().numpy()[0][valid_phones].tolist()\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def predict_batch(self, audio_files, text_files=None):\n",
        "        \"\"\"\n",
        "        Predict GOP scores for multiple audio files\n",
        "\n",
        "        Args:\n",
        "            audio_files: list of audio file paths\n",
        "            text_files: list of text file paths (optional)\n",
        "\n",
        "        Returns:\n",
        "            results: list of prediction results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, audio_file in enumerate(audio_files):\n",
        "            text_file = text_files[i] if text_files else None\n",
        "            result = self.predict_single_audio(audio_file, text_file)\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_results(self, results, output_file):\n",
        "        \"\"\"Save results to JSON file\"\"\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of GOP inference\"\"\"\n",
        "\n",
        "    # Initialize inference class\n",
        "    model_path = \"path/to/your/best_audio_model.pth\"  # Update this path\n",
        "    gop_inference = GOPInference(\n",
        "        model_path=model_path,\n",
        "        am='paiia',  # or 'librispeech', 'paiib'\n",
        "        device='cuda'\n",
        "    )\n",
        "\n",
        "    # Single audio prediction\n",
        "    audio_file = \"/content/alar_eng_0_clean_wavlm_features.wav\"  # Update this path\n",
        "    text_file = \"path/to/your/text.txt\"    # Optional, update this path\n",
        "\n",
        "    print(\"Predicting GOP scores for single audio...\")\n",
        "    results = gop_inference.predict_single_audio(audio_file, text_file)\n",
        "\n",
        "    print(\"\\nUtterance-level scores:\")\n",
        "    for score_type, score in results['utterance_scores'].items():\n",
        "        print(f\"  {score_type}: {score:.3f}\")\n",
        "\n",
        "    print(f\"\\nPhone-level scores: {len(results['phone_scores'])} phones\")\n",
        "    print(f\"Average phone score: {np.mean(results['phone_scores']):.3f}\")\n",
        "\n",
        "    print(f\"\\nWord-level scores: {len(results['word_scores']['accuracy'])} words\")\n",
        "    print(f\"Average word accuracy: {np.mean(results['word_scores']['accuracy']):.3f}\")\n",
        "\n",
        "    # Batch prediction\n",
        "    audio_files = [\n",
        "        \"path/to/audio1.wav\",\n",
        "        \"path/to/audio2.wav\",\n",
        "        # Add more files as needed\n",
        "    ]\n",
        "\n",
        "    print(\"\\nPredicting GOP scores for batch...\")\n",
        "    batch_results = gop_inference.predict_batch(audio_files)\n",
        "\n",
        "    # Save results\n",
        "    gop_inference.save_results(batch_results, \"gop_predictions.json\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qv5W_9rQhQLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "if torch.cuda.is_available() == False:\n",
        "    raise ValueError('GPU is not enabled. Please go to top menu - edit - notebook settings -hardware accelerator - GPU')\n",
        "\n",
        "am = args.am\n",
        "print('now train with {:s} acoustic models'.format(am))\n",
        "feat_dim = {'librispeech':84, 'paiia':86, 'paiib': 88}\n",
        "input_dim=feat_dim[am]\n",
        "\n",
        "# Specify your custom model save directory\n",
        "custom_model_dir = \"/content/drive/MyDrive/gopt_models\"  # Change this to your desired path\n",
        "# Or use: custom_model_dir = \"./saved_models\"  # For local directory\n",
        "\n",
        "# Create the custom directory if it doesn't exist\n",
        "if not os.path.exists(custom_model_dir):\n",
        "    os.makedirs(custom_model_dir)\n",
        "    print(f\"Created model directory: {custom_model_dir}\")\n",
        "\n",
        "# Override the experiment directory in args\n",
        "args.exp_dir = custom_model_dir\n",
        "print(f\"Model will be saved to: {args.exp_dir}\")\n",
        "\n",
        "# nowa is the best models used in this work\n",
        "if args.model == 'gopt':\n",
        "    print('now train a GOPT models')\n",
        "    audio_mdl = GOPT(embed_dim=args.embed_dim, num_heads=args.goptheads, depth=args.goptdepth, input_dim=input_dim)\n",
        "\n",
        "tr_dataset = GoPDataset('train', am=am)\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "te_dataset = GoPDataset('test', am=am)\n",
        "te_dataloader = DataLoader(te_dataset, batch_size=2500, shuffle=False)\n",
        "\n",
        "# The experiment directory is already created above\n",
        "print(f\"Training model and saving to: {args.exp_dir}\")\n",
        "train(audio_mdl, tr_dataloader, te_dataloader, args)\n",
        "\n",
        "print(f\"Training completed. Model saved in: {args.exp_dir}\")"
      ],
      "metadata": {
        "id": "LoOJk1FZnmLS",
        "outputId": "ffbe2411-8f30-4e71-a223-3f2512415353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now train with paiia acoustic models\n",
            "Created model directory: /content/drive/MyDrive/gopt_models\n",
            "Model will be saved to: /content/drive/MyDrive/gopt_models\n",
            "now train a GOPT models\n",
            "Training model and saving to: /content/drive/MyDrive/gopt_models\n",
            "running on cuda\n",
            "Total parameter number is : 26.625 k\n",
            "Total trainable parameter number is : 26.625 k\n",
            "current #steps=0, #epochs=0\n",
            "start training...\n",
            "start validation of epoch 0\n",
            "new best phn mse 0.099, now saving predictions.\n",
            "Phone: Test MSE: 0.099, CORR: 0.473\n",
            "Utterance:, ACC: 0.369, COM: 0.008, FLU: -0.055, PROC: 0.027, Total: 0.207\n",
            "Word:, ACC: 0.423, Stress: -0.009, Total: 0.375\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 1\n",
            "new best phn mse 0.095, now saving predictions.\n",
            "Phone: Test MSE: 0.095, CORR: 0.542\n",
            "Utterance:, ACC: 0.523, COM: -0.005, FLU: 0.093, PROC: 0.314, Total: 0.332\n",
            "Word:, ACC: 0.466, Stress: 0.078, Total: 0.490\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 2\n",
            "new best phn mse 0.086, now saving predictions.\n",
            "Phone: Test MSE: 0.086, CORR: 0.571\n",
            "Utterance:, ACC: 0.578, COM: -0.007, FLU: 0.260, PROC: 0.490, Total: 0.481\n",
            "Word:, ACC: 0.504, Stress: 0.071, Total: 0.530\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 3\n",
            "new best phn mse 0.084, now saving predictions.\n",
            "Phone: Test MSE: 0.084, CORR: 0.588\n",
            "Utterance:, ACC: 0.655, COM: -0.034, FLU: 0.350, PROC: 0.580, Total: 0.674\n",
            "Word:, ACC: 0.539, Stress: 0.061, Total: 0.559\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 4\n",
            "new best phn mse 0.081, now saving predictions.\n",
            "Phone: Test MSE: 0.081, CORR: 0.605\n",
            "Utterance:, ACC: 0.664, COM: -0.120, FLU: 0.615, PROC: 0.596, Total: 0.682\n",
            "Word:, ACC: 0.569, Stress: 0.047, Total: 0.589\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 5\n",
            "new best phn mse 0.079, now saving predictions.\n",
            "Phone: Test MSE: 0.079, CORR: 0.636\n",
            "Utterance:, ACC: 0.669, COM: -0.111, FLU: 0.612, PROC: 0.598, Total: 0.686\n",
            "Word:, ACC: 0.588, Stress: 0.055, Total: 0.602\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 6\n",
            "new best phn mse 0.074, now saving predictions.\n",
            "Phone: Test MSE: 0.074, CORR: 0.649\n",
            "Utterance:, ACC: 0.680, COM: -0.082, FLU: 0.629, PROC: 0.610, Total: 0.698\n",
            "Word:, ACC: 0.579, Stress: 0.121, Total: 0.594\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 7\n",
            "new best phn mse 0.073, now saving predictions.\n",
            "Phone: Test MSE: 0.073, CORR: 0.658\n",
            "Utterance:, ACC: 0.685, COM: -0.001, FLU: 0.637, PROC: 0.606, Total: 0.703\n",
            "Word:, ACC: 0.577, Stress: 0.111, Total: 0.594\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 8\n",
            "new best phn mse 0.071, now saving predictions.\n",
            "Phone: Test MSE: 0.071, CORR: 0.665\n",
            "Utterance:, ACC: 0.694, COM: 0.024, FLU: 0.638, PROC: 0.618, Total: 0.703\n",
            "Word:, ACC: 0.580, Stress: 0.149, Total: 0.592\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 9\n",
            "new best phn mse 0.071, now saving predictions.\n",
            "Phone: Test MSE: 0.071, CORR: 0.670\n",
            "Utterance:, ACC: 0.695, COM: -0.027, FLU: 0.637, PROC: 0.605, Total: 0.707\n",
            "Word:, ACC: 0.580, Stress: 0.090, Total: 0.593\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 10\n",
            "new best phn mse 0.070, now saving predictions.\n",
            "Phone: Test MSE: 0.070, CORR: 0.673\n",
            "Utterance:, ACC: 0.701, COM: 0.095, FLU: 0.644, PROC: 0.634, Total: 0.707\n",
            "Word:, ACC: 0.593, Stress: 0.153, Total: 0.605\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 11\n",
            "new best phn mse 0.069, now saving predictions.\n",
            "Phone: Test MSE: 0.069, CORR: 0.677\n",
            "Utterance:, ACC: 0.707, COM: 0.073, FLU: 0.648, PROC: 0.636, Total: 0.713\n",
            "Word:, ACC: 0.588, Stress: 0.145, Total: 0.600\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 12\n",
            "Phone: Test MSE: 0.069, CORR: 0.679\n",
            "Utterance:, ACC: 0.713, COM: 0.046, FLU: 0.656, PROC: 0.643, Total: 0.718\n",
            "Word:, ACC: 0.592, Stress: 0.146, Total: 0.604\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 13\n",
            "Phone: Test MSE: 0.070, CORR: 0.677\n",
            "Utterance:, ACC: 0.714, COM: 0.050, FLU: 0.658, PROC: 0.648, Total: 0.719\n",
            "Word:, ACC: 0.592, Stress: 0.155, Total: 0.603\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 14\n",
            "Phone: Test MSE: 0.069, CORR: 0.681\n",
            "Utterance:, ACC: 0.708, COM: -0.000, FLU: 0.647, PROC: 0.636, Total: 0.713\n",
            "Word:, ACC: 0.600, Stress: 0.140, Total: 0.612\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 15\n",
            "new best phn mse 0.069, now saving predictions.\n",
            "Phone: Test MSE: 0.069, CORR: 0.682\n",
            "Utterance:, ACC: 0.716, COM: 0.033, FLU: 0.659, PROC: 0.651, Total: 0.720\n",
            "Word:, ACC: 0.599, Stress: 0.144, Total: 0.614\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 16\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.717, COM: 0.026, FLU: 0.659, PROC: 0.650, Total: 0.720\n",
            "Word:, ACC: 0.598, Stress: 0.141, Total: 0.609\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 17\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.718, COM: 0.020, FLU: 0.659, PROC: 0.651, Total: 0.721\n",
            "Word:, ACC: 0.598, Stress: 0.140, Total: 0.610\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 18\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.685\n",
            "Utterance:, ACC: 0.718, COM: 0.011, FLU: 0.661, PROC: 0.653, Total: 0.721\n",
            "Word:, ACC: 0.598, Stress: 0.148, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 19\n",
            "Phone: Test MSE: 0.070, CORR: 0.678\n",
            "Utterance:, ACC: 0.720, COM: 0.030, FLU: 0.663, PROC: 0.656, Total: 0.721\n",
            "Word:, ACC: 0.590, Stress: 0.157, Total: 0.599\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 20\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.719, COM: 0.010, FLU: 0.663, PROC: 0.654, Total: 0.721\n",
            "Word:, ACC: 0.599, Stress: 0.126, Total: 0.611\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 21\n",
            "Phone: Test MSE: 0.068, CORR: 0.685\n",
            "Utterance:, ACC: 0.720, COM: 0.010, FLU: 0.663, PROC: 0.655, Total: 0.722\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.609\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 22\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.721, COM: 0.016, FLU: 0.664, PROC: 0.657, Total: 0.723\n",
            "Word:, ACC: 0.598, Stress: 0.152, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 23\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.721, COM: 0.019, FLU: 0.665, PROC: 0.658, Total: 0.723\n",
            "Word:, ACC: 0.596, Stress: 0.148, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 24\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: 0.015, FLU: 0.666, PROC: 0.659, Total: 0.724\n",
            "Word:, ACC: 0.596, Stress: 0.151, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 25\n",
            "new best phn mse 0.068, now saving predictions.\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: 0.005, FLU: 0.667, PROC: 0.660, Total: 0.724\n",
            "Word:, ACC: 0.599, Stress: 0.149, Total: 0.610\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 26\n",
            "Phone: Test MSE: 0.069, CORR: 0.682\n",
            "Utterance:, ACC: 0.722, COM: 0.010, FLU: 0.667, PROC: 0.660, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.152, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 27\n",
            "Phone: Test MSE: 0.068, CORR: 0.685\n",
            "Utterance:, ACC: 0.720, COM: -0.010, FLU: 0.664, PROC: 0.658, Total: 0.722\n",
            "Word:, ACC: 0.598, Stress: 0.140, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 28\n",
            "new best phn mse 0.067, now saving predictions.\n",
            "Phone: Test MSE: 0.067, CORR: 0.684\n",
            "Utterance:, ACC: 0.720, COM: -0.002, FLU: 0.665, PROC: 0.658, Total: 0.723\n",
            "Word:, ACC: 0.597, Stress: 0.142, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 29\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: 0.006, FLU: 0.667, PROC: 0.661, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.149, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 30\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: 0.005, FLU: 0.667, PROC: 0.661, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.150, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 31\n",
            "Phone: Test MSE: 0.067, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: 0.000, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 32\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: 0.003, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.596, Stress: 0.148, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 33\n",
            "Phone: Test MSE: 0.069, CORR: 0.681\n",
            "Utterance:, ACC: 0.722, COM: 0.010, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.594, Stress: 0.152, Total: 0.604\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 34\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: 0.004, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.596, Stress: 0.148, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 35\n",
            "Phone: Test MSE: 0.069, CORR: 0.681\n",
            "Utterance:, ACC: 0.722, COM: 0.009, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.595, Stress: 0.150, Total: 0.605\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 36\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.722, COM: 0.004, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.596, Stress: 0.148, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 37\n",
            "Phone: Test MSE: 0.067, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.001, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 38\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.001, FLU: 0.667, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 39\n",
            "new best phn mse 0.067, now saving predictions.\n",
            "Phone: Test MSE: 0.067, CORR: 0.685\n",
            "Utterance:, ACC: 0.722, COM: -0.007, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.599, Stress: 0.142, Total: 0.609\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 40\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.002, FLU: 0.668, PROC: 0.662, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 41\n",
            "Phone: Test MSE: 0.067, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.668, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 42\n",
            "Phone: Test MSE: 0.067, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.668, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.144, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 43\n",
            "Phone: Test MSE: 0.068, CORR: 0.683\n",
            "Utterance:, ACC: 0.723, COM: 0.001, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.596, Stress: 0.147, Total: 0.606\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 44\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 45\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.723, COM: -0.002, FLU: 0.669, PROC: 0.663, Total: 0.725\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 46\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 47\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.002, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 48\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.003, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.598, Stress: 0.145, Total: 0.608\n",
            "-------------------validation finished-------------------\n",
            "start validation of epoch 49\n",
            "Phone: Test MSE: 0.068, CORR: 0.684\n",
            "Utterance:, ACC: 0.722, COM: -0.002, FLU: 0.669, PROC: 0.663, Total: 0.724\n",
            "Word:, ACC: 0.597, Stress: 0.146, Total: 0.607\n",
            "-------------------validation finished-------------------\n",
            "Training completed. Model saved in: /content/drive/MyDrive/gopt_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6HHrglGVnnWJ",
        "outputId": "a30ff2be-f044-47b0-9e03-dc7abd50b734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}